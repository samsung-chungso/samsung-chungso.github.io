---
published: true
layout: splash
classes:
  - landing
  - dark-theme
title: '[CS231n 강의노트] Assignment 1: Q1. k-Nearest Neighbor classifier'
use_math: true
tags:
- image classification
- cs231n
- kNN classifier
---

> Standford University의 cs231n 강의를 정리한 내용입니다.
> 과제 1의 풀이를 정리해보려고 합니다.


# Q1. k-Nearest Neighbor classifier
## k-Nearest Neighbor (kNN) exercise
### knn.ipynb

#### classifier 이용해서 이미지의 라벨 예측하기
kNN classifier는 두 단계로 구성되어 있습니다.
- Train 단계에서 classifier는 training 데이터(CIFAR-10)를 받고, 모두 기억합니다.
- Test 단계에서, kNN classifier는 모든 테스트 이미지를 training 데이터인 이미지들과 비교하고, training 데이터의 최근접 라벨로 변환합니다.

Train 단계이자, Test 단계 전에 training 데이터(CIFAR-10)의 차원을 변경해줍니다. training 데이터는 이미지 데이터로 4차원으로 구성되어서 X_train, X_test에 저장됩니다. training 데이터의 라벨 정보는 y_train, y_test에 1차원으로 저장됩니다.

<img width="351" alt="image" src="https://github.com/samsung-chungso/samsung-chungso.github.io/assets/103614665/47ca5cd0-a1cd-486b-8da1-758dff213022">


픽셀값을 넘파이 배열로 불러올 때에는 reshape 함수를 이용해 차원을 변경해줍니다. 기존에 2

Test 단계는 두 과정으로 나눠볼 수 있겠습니다.

  1. 모든 Train, Test 데이터의 거리값을 계산합니다. L2 distance인 유클리디안 거리 공식을 사용하겠습니다.
        $$d_2(I_1, I_2) = \sqrt{\displaystyle\sum_{p}(I_1^P-I_2^P)^2}$$
  2. 계산된 거리값을 기반으로 Test 데이터와 가까운 k개의 Train 데이터를 찾고, 라벨값의 다수결을 뽑습니다.

Train, Test 데이터의 거리값을 3가지 방식으로 계산해보겠습니다. knn.ipynb 에서 compute_distances_two_loops()를 사용하고 함수가 정의된 파일은 k_nearest_neighbor.py 입니다.  

X로 Train 데이터를 받고, X_train으로 Train 데이터를 받아서 픽셀로 나눠진 각 지점 값을 넘파이 배열로 불러옵니다.

**2개 루프로 계산하기**
**compute_distances_two_loops()**

*힌트*
*i번째 Test 데이터의 지점과 j번째 Train 데이터의 지점의 L2 distance를 계산하고 dists[i,j]에 저장하기*

    def compute_distances_two_loops(self, X):
        num_test = X.shape[0]
        num_train = self.X_train.shape[0]
        dists = np.zeros((num_test, num_train))
        for i in range(num_test):
            for j in range(num_train): 
                distance = np.sqrt(np.sum(np.square(self.X_train[j]- X[i])))
                dists[i,j] = distance
        return dists

**1개 루프로 계산하기**
**compute_distances_one_loop()**

*힌트*
*i번째 Test 데이터의 지점과 모든 Train 데이터 지점들의 L2 distance를 계산하고 dist[i,:]에 저장하기*

    def compute_distances_one_loop(self, X):
        num_test = X.shape[0]
        num_train = self.X_train.shape[0]
        dists = np.zeros((num_test, num_train))
        for i in range(num_test):
            distance = np.sqrt(np.sum(np.square(self.X_train - X[i,:]), axis = 1))
            dists[i, :] = distance
        return dists

**루프 없이 계산하기**
**compute_distance_no_loops()**

*힌트*
*행렬곱과 두개의 broadcast 합을 이용해서 구하시오.*
    def compute_distances_no_loops(self, X):
        num_test = X.shape[0]
        num_train = self.X_train.shape[0]
        dists = np.zeros((num_test, num_train))

#### Cross-validation 구현해보기

*힌트*
*training data를 fold 안에 쪼개서 넣습니다. numpy array_split 함수 사용하면 쉽습니다*
    X_train_folds = np.array_split(X_train, num_folds)
    y_train_folds = np.array_split(y_train, num_folds)

*힌트*
*쪼갠 fold 갯수 k만큼 k-nearest-neighbor 알고리즘 실행시켜서 최적의 k를 찾습니다. k-1개의 케이스에 대해서는 training data를 사용하고, 마지막 한개에 대해서는 validation data를 사용하여 k값별 정확도를 딕셔너리에 넣어 줍니다.*

#### 출처
CS231n 강의:(https://cs231n.github.io/assignments2024/assignment1/)
+ Assignment tutorial 올려주신 조교님이 한국계이신 것 같네요

J Lee 풀이
