---
published: true
layout: splash
classes:
  - landing
  - dark-theme
title: '[CS231n 강의노트] Sigmoid Function 단점: not zero-centered'
use_math: true
tags:
- image classification
- cs231n
- kNN classifier
---

> Standford University의 cs231n 강의를 정리한 내용입니다.
> Sigmoid Function의 단점을 정리해보려고 합니다.


# Lecture6. Training Neural Network _1_
## Activation Function
### Sigmoid Function

Sigmoid function은 입력값을 0부터 1까지 값으로 밀어넣어버리는 함수입니다. 대부분의 입력값들이 0과 1로 갈라쳐지므로 해석력이 좋아 과거에 많이 쓰였던 activation function 활성함수입니다.
img width="351" alt="image" src="https://github.com/samsung-chungso/samsung-chungso.github.io/blob/master/assets/images/sigmoid.png">
$$ /sigma(x) = /frac{1}{(1+/exp(-x)} $$

그러나 3가지 문제점이 있습니다. 
1. 기울기가 0으로 수렴하는 구간들 존재합니다. Backpropagation에서 계산되는 미분 값이 0이 되면서, vanishing gradient 문제가 발생합니다.
2. Sigmoid function의 결과값은 무조건 양수로 나옵니다. 양수와 음수가 골고루 출력되는 zero-centered 함수가 아닌 것이죠. 
3. exp() 연산은 단순한 연산이 아니라 리소스가 더 많이 드는 연산입니다. 

그 중 2번째 문제에 대해 자세히 살펴보고자 합니다. 
#### 2. Sigmoid outputs are not zero-centered


#### 출처
- CS231n 강의:(https://youtu.be/wEoyxE0GP2M?si=goQx8ZGdMzIF-i8w)
- 혜규님과 chat GPT 풀이
- Steve-Lee's Deep Insight : (https://deepinsight.tistory.com/113)
- Desmos 그래프 그리기

