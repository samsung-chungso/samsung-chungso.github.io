---
published: true
layout: splash
classes:
  - landing
  - dark-theme
title: '[CS231n 강의노트] Sigmoid Function 단점: not zero-centered'
use_math: true
tags:
- image classification
- cs231n
- kNN classifier
---

> Standford University의 cs231n 강의를 정리한 내용입니다.
> Sigmoid Function의 단점을 정리해보려고 합니다.


# Lecture6. Training Neural Network _1_
## Activation Function
### Sigmoid Function

Sigmoid function은 입력값을 0부터 1까지 값으로 밀어넣어버리는 함수입니다. 대부분의 입력값들이 0과 1로 갈라쳐지므로 해석력이 좋아 과거에 많이 쓰였던 activation function 활성함수입니다.

![sigmoid.png](/assets/images/sigmoid.png)

$$/sigma(x) = /frac{1}{(1+/exp(-x)}$$

그러나 3가지 문제점이 있습니다. 
1. 기울기가 0으로 수렴하는 구간들 존재합니다. Backpropagation에서 계산되는 미분 값이 0이 되면서, vanishing gradient 문제가 발생합니다.
2. Sigmoid function의 결과값은 무조건 양수로 나옵니다. 양수와 음수가 골고루 출력되는 zero-centered 함수가 아닌 것이죠. 
3. exp() 연산은 단순한 연산이 아니라 리소스가 더 많이 드는 연산입니다. 

그 중 2번째 문제에 대해 자세히 살펴보고자 합니다. 
#### 2. Sigmoid outputs are not zero-centered
Lecture 4. Introduction to Neural Networks의 backpropagation 설명과 연결지어 이해하면 도움이 되실 겁니다. Backpropagation은 역방향으로 미분값을 계산하여 weight값 w를 업데이트하는 방식입니다. 
업데이트 식은 $w' = w - \eta\frac{\partialL}{\partialw}$ 이며, 여기서 미분값은 chain rule에 의하여 아래와 같이 단계별 미분값을 곱해 계산 및 표현할 수 있습니다.
$$\frac{\partial{L}}{\partial{w}}=\frac{\partial{L}}{\partial{f}}\frac{\partial{f}}{\partial{w}}$$ 


여기서 $f=wx+b$이므로 $frac{\partial{f}}{\partial{w}} = x$가 되고, 

![notzero-centered.jpeg](/assets/images/notzero-centered.jpeg)

#### 출처
- CS231n 강의:(https://youtu.be/wEoyxE0GP2M?si=goQx8ZGdMzIF-i8w)
- 혜규님과 chat GPT 풀이
- Steve-Lee's Deep Insight : (https://deepinsight.tistory.com/113)
- Desmos 그래프 그리기

