---
published: true
layout: splash
classes:
  - landing
  - dark-theme
title: '[CS231n 강의노트] Sigmoid Function 단점: not zero-centered'
use_math: true
tags:
- image classification
- cs231n
- kNN classifier
---

> Standford University의 cs231n 강의를 정리한 내용입니다.
> Sigmoid Function의 단점을 정리해보려고 합니다.


# Lecture6. Training Neural Network 1
## Activation Function
### Sigmoid Function

Sigmoid function은 입력값을 0부터 1까지 값으로 밀어넣어버리는 함수입니다. 대부분의 입력값들이 0과 1로 갈라쳐지므로 해석력이 좋아 과거에 많이 쓰였던 activation function 활성함수입니다.

![sigmoid.png](/assets/images/sigmoid.png)

$$\sigma(x) = \frac{1}{(1+e^{-x})}$$

그러나 3가지 문제점이 있습니다. 
1. 기울기가 0으로 수렴하는 구간들 존재합니다. Backpropagation에서 계산되는 미분 값이 0이 되면서, vanishing gradient 문제가 발생합니다.
2. Sigmoid function의 결과값은 무조건 양수로 나옵니다. 양수와 음수가 골고루 출력되는 zero-centered 함수가 아닌 것이죠. 
3. exp() 연산은 단순한 연산이 아니라 리소스가 더 많이 드는 연산입니다. 

그 중 2번째 문제에 대해 자세히 살펴보고자 합니다. 
#### 2. Sigmoid outputs are not zero-centered
Lecture 4. Introduction to Neural Networks의 backpropagation 설명과 연결지어 이해하면 도움이 되실 겁니다. Backpropagation은 역방향으로 미분값을 계산하여 weight값 w를 업데이트하는 방식입니다. 
업데이트 식은 $w' = w - \eta\frac{\partial{L}}{\partial{w}}$ 이며, 여기서 미분값은 chain rule에 의하여 아래와 같이 단계별 미분값을 곱해 계산 및 표현할 수 있습니다.

$$\frac{\partial{L}}{\partial{w}}=\frac{\partial{L}}{\partial{f}}\cdot\frac{\partial{f}}{\partial{w}}$$ 


위 식에서 $\frac{\partial{f}}{\partial{w}} = X$가 되고, X는 sigmoid의 not zero-centered 성질에 의해 양의 값만 가질 수도 있습니다. 
X가 언제나 양수라면, w 업데이트 방향은 상위 단계의 미분값 upstream gradient  $\frac{\partial{L}}{\partial{f}}$에 의해서 업데이트 방향이 결정됩니다. 업데이트가 진행되는 여러 weight w가 있을 경우 chain rule로 gradient들의 곱이므로, 일부 w들의 backpropagation에서 upstream part는 공유하고 있습니다. 모두 같은 방향으로의 업데이트만 존재하게 되므로, 두개의 w만 놓고 생각하면 $w_1, w_2$가 함께 증가하거나 함께 감소하게 됩니다. 2차원 평면에서 표현해보면 1사분면, 3사분면의 방향으로 밖에 전진하지 못하는 것이죠.

![direction1.jpeg](/assets/images/direction1.jpeg){: width="150" height="150"}

같은 방향으로의 업데이트를 조금 풀어서 아래와 같이 작성했습니다. Local gradient는 일부 단계의 미분값 gradient를 계산한 것을 의미하므로 초록색 형광펜 칠한 부분과 하늘색 형광펜 칠한 부분 모두 local gradient로 부를 수 있고, 그 중 상위 단계인 gradient를 upstream gradient라고 칭하였습니다.

![notzero-centered.jpeg](/assets/images/notzero-centered.jpeg)

두개의 w값이 2차원 평면 위에 있다고 가정하고 보면, 정해진 방향 안에서 w 값을 동시에 업데이트하다 보니 zig zag path를 그리게 되는 것입니다.

![direction2.jpeg](/assets/images/direction2.jpeg){: width="150" height="150"}

#### 출처
- CS231n 강의:(https://youtu.be/wEoyxE0GP2M?si=goQx8ZGdMzIF-i8w)
- 혜규님과 chat GPT 풀이
- Steve-Lee's Deep Insight : (https://deepinsight.tistory.com/113)
- Desmos 그래프 그리기
- https://stats.stackexchange.com/questions/237169/why-are-non-zero-centered-activation-functions-a-problem-in-backpropagation

