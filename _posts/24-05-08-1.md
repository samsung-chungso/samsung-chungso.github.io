---
publised: true
layout: splash
classes:
  - landing
  - dark-theme
title: '[CS231n 강의노트] Lecture2'
---

> Standford University의 cs231n 강의를 정리한 내용입니다.
# Lecture2. Image Classification
## Image Classification
#### motivation & example
Image classification은 전체 이미지를 보고 특정 카테고리(예. 고양이, 자동차)로 레이블을 분류하는 것입니다. 컴퓨터 비전 측면에서 이미지는 어떤 의미를 가진다기보다는 숫자로 구성된 큰 3차원 배열입니다. 너비 Width x 높이 Height x 3 으로 구성되어 있고, 각 픽셀에 들어가는 수는 0(검정)부터 255(하얀색)으로 구성되어 있으며 채널 3(x 3)은 Red, Green, Blue입니다.
#### chanllenges
컴퓨터 비전 입장에서 이미지는 수로 이뤄진 배열의 집합이기 때문에 사람처럼 이미지를 보고 인식하는 것은 어렵습니다. 특히 다음 7가지 챌린지가 있습니다.
- Viewpoint variation: 촬영 각도
- Scale variation: 꼭 사진 크기가 아니더라도 객체 크기의 다양성
- Deformation: 다른 자세
- Occlusion: 살짝 가려짐
- Illumination conditions: 조명
- Background clutter: 배경과 보호색을 이룸
- Intra-class variation: 동일한 객체도 다양한 생김새

#### data-driven approach
챌린지를 해결하고자 데이터 기반으로 접근했던 방식을 알아보겠습니다. 만약 데이터 기반이 아닌 특정 고양이 이미지의 edge를 인식하는 알고리즘을 만든다 라고 한다면, 조금이라도 달라진 고양이 이미지에 대해 또는 위 챌린지를 가진 이미지에 대해 알고리즘은 동작하지 않을 것입니다. 따라서 모든 객체마다 이미지를 인식할 수 있는 알고리즘을 구현하는 것 대신 어린아이에게 가르치듯이 특정 클래스(예. 고양이)에 해당하는 많은 이미지를 보여주는 방식을 택하고, 클래스가 라벨링된 이미지를 축적했습니다. 
#### image classification pipeline
Image classification에서 파이프라인은 다음과 같이 형성되었습니다. 
- Input: N개의 이미지를 가지고 K개의 다른 클래스가 라벨링되어 있습니다. 그리고 머신러닝에서 사용한 단어와 동일하게 training set이라고 부릅니다.
- Learning: 개별 클래스가 어떻게 생겼는지 training set기반으로 학습합니다. 이 과정을 training classifier 또는 모델을 학습한다(learning a model)고 합니다.
- Evaluation: 학습한 모델(혹은 classifier)의 라벨 예측 성능을 평가하는 단계가 마지막에 있습니다. 실제 라벨(ground truth)과 비교했을 때 얼마나 잘 예측하는 지 판단합니다.

## Nearest Neighbor Classifier
Machine Learning 기반의 classifier는 Nearest Neighbor입니다. 모든 샘플 데이터를 기억하고, 비슷한 이미지를 픽셀값 기반으로 계산하는 방식으로 추천하는 classifier는 아니지만, 이미지 분류 문제에 대한 기초적인 접근을 위해 코드도 구현해보고 배울 것입니다.
### Example image classification dataset: CIFAR-10

### L1 vs. L2

## k-Nearest Neighbor Classifier


## Validation sets for Hyperparameter tuning
### Cross-validation
### In practice
### Pros and Cons of Nearest Neighbor classifier


#### 출처
CS231n 강의:https://www.youtube.com/watch?v=OoUX-nOEjG0&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=2
